{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b2f66c-777f-47a4-8a8b-df71f5414ef3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Goal\n",
    "Apply an unsupervised approach to thematic analysis of pain mentions. Compare/contrast these with our LLM biopsychosocial pain dimensions. This can show us if we’re comprehensive in our approach or if there are others that we aren’t catching—like other dimensions of pain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5fdfcc-7f1d-4e45-87bb-4c41c5e5d530",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df5669-cb66-4649-99a6-5fb86d0e16f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bertopic sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240c14a-5bcc-4b6c-81e2-3dac32a137aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install hvplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056a2bc1-17c3-4a9c-8cc0-a9ff4c411f62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb7e4d-ead8-43aa-958d-c4c159cac521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import ast\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import panel as pn\n",
    "import hvplot.pandas\n",
    "from IPython.display import display\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from bertopic import BERTopic\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import plotly.express as px\n",
    "from typing import List, Tuple, Dict\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d47613f-cf45-4bad-8da2-3878850f9023",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d43bd-3fb3-4989-8b8b-74a9b26e5136",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The dataset being read below contains all sentences with matches in the pain lexicon, and only the matched sentences.\n",
    "Pain Lexicons: https://docs.google.com/spreadsheets/d/1GoV-g_38ntJ4X6q3TmCIoWazb2JPSu-QptFDRiP5sYo\n",
    "'''\n",
    "\n",
    "df = pd.read_csv('reddit-data/matched-datasets/pain_lexicon_matched_subreddit_lupus_posts_full.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e7cdd5-967c-4bbd-a67b-d6a2bd31acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"shape of dataset: {df.shape}\")\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d97469-bcc7-4734-b429-de24cb0babd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.extension()\n",
    "frequency_counter = Counter()\n",
    "for entry in df['matches']:\n",
    "    parsed_list = ast.literal_eval(entry)\n",
    "    frequency_counter.update(parsed_list)\n",
    "terms = [term for term, frequency in frequency_counter.most_common()]\n",
    "frequencies = [frequency for term, frequency in frequency_counter.most_common()]\n",
    "data = pd.DataFrame({'Term': terms, 'Frequency': frequencies})\n",
    "slider = pn.widgets.IntSlider(name='Frequency Threshold', \n",
    "                            start=1, \n",
    "                            end=max(frequencies), \n",
    "                            step=1, \n",
    "                            value=10)\n",
    "\n",
    "# Dynamic table and plot with increased size\n",
    "@pn.depends(slider)\n",
    "def filter_data(threshold):\n",
    "    filtered_data = data[data['Frequency'] >= threshold]\n",
    "    plot = filtered_data.hvplot.bar(\n",
    "        x='Term', \n",
    "        y='Frequency', \n",
    "        rot=45, \n",
    "        title=f'Terms with Frequency >= {threshold}',\n",
    "        height=400,  # Increased height\n",
    "        width=1000,   # Increased width\n",
    "        fontsize={'title': 16, 'labels': 12, 'xticks': 10, 'yticks': 10}  # Larger fonts\n",
    "    )\n",
    "    return pn.Column(filtered_data, plot)\n",
    "dashboard = pn.Column(slider, filter_data)\n",
    "display(dashboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d65e6df-5375-4fbe-ad6f-aeee893a583a",
   "metadata": {},
   "source": [
    "# Single Sentence Thematic Analysis using BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba99d099-8aa9-47dd-8ec2-bed44f22c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PainTopicAnalyzer:\n",
    "    def __init__(self, output_folder: str = \"single_sentence_pain_topic_analysis\"):\n",
    "        self.output_folder = output_folder\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        self.pain_dimensions = {\n",
    "            'biological': ['location', 'severity', 'duration', 'sex', 'gender', 'age', 'comorbidities'],\n",
    "            'psychological': ['affective', 'cognitive', 'behavioral', 'existential', 'spiritual'],\n",
    "            'social': ['functional', 'economic', 'sociocultural'],\n",
    "            'nociplastic': ['fatigue', 'brain fog', 'widespread pain', 'sensitivity'],\n",
    "            'management': ['medication', 'treatment', 'therapy', 'management']\n",
    "        }\n",
    "    def load_and_prepare_data(self, filepath: str) -> Tuple[pd.DataFrame, List[str]]:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"Loaded {len(df)} rows\")\n",
    "        \n",
    "        df['sentence'] = df['sentence'].astype(str)\n",
    "        df = df[df['sentence'].str.strip().str.len() > 0]\n",
    "        \n",
    "        context_df = df.copy()\n",
    "        context_df['matched_pain_terms'] = df['matches']\n",
    "        context_df['post_date'] = pd.to_datetime(df['created_date_cleaned'])\n",
    "        \n",
    "        docs = df['sentence'].tolist()\n",
    "        \n",
    "        print(f\"Prepared {len(docs)} sentences for analysis\")\n",
    "        return context_df, docs\n",
    "\n",
    "    def map_topics_to_dimensions(self, topic_words: Dict[int, List[str]]) -> Dict:\n",
    "        \"\"\"Map discovered topics to pain dimensions framework.\"\"\"\n",
    "        topic_dimension_mapping = {}\n",
    "        \n",
    "        for topic_id, words in topic_words.items():\n",
    "            matched_dimensions = []\n",
    "            \n",
    "            for dimension, keywords in self.pain_dimensions.items():\n",
    "                if any(keyword in ' '.join(words).lower() for keyword in keywords):\n",
    "                    matched_dimensions.append(dimension)\n",
    "            \n",
    "            topic_dimension_mapping[topic_id] = {\n",
    "                'topic_words': words,\n",
    "                'matched_dimensions': matched_dimensions,\n",
    "                'unknown_dimension': len(matched_dimensions) == 0\n",
    "            }\n",
    "        \n",
    "        return topic_dimension_mapping\n",
    "        \n",
    "    def save_analysis_results(self, topic_words: Dict[int, List[str]], \n",
    "                            topic_dimension_mapping: Dict, \n",
    "                            context_df: pd.DataFrame,\n",
    "                            base_filename: str):\n",
    "        # Save topic-dimension mapping\n",
    "        mapping_df = pd.DataFrame([\n",
    "            {\n",
    "                'Topic': topic_id,\n",
    "                'Words': ', '.join(info['topic_words']),\n",
    "                'Matched_Dimensions': ', '.join(info['matched_dimensions']),\n",
    "                'New_Dimension': info['unknown_dimension']\n",
    "            }\n",
    "            for topic_id, info in topic_dimension_mapping.items()\n",
    "        ])\n",
    "        \n",
    "        mapping_df.to_csv(f\"{base_filename}_dimension_mapping.csv\", index=False)\n",
    "        \n",
    "        # Save sentences with their topics and matched pain terms\n",
    "        analysis_df = context_df[['sentence', 'matches', 'topic', 'post_id', 'created_date_cleaned']]\n",
    "        analysis_df.to_csv(f\"{base_filename}_sentences_with_topics.csv\", index=False)\n",
    "        \n",
    "        # Save summary statistics\n",
    "        with open(f\"{base_filename}_analysis_summary.txt\", 'w') as f:\n",
    "            f.write(\"Pain Topic Analysis Summary\\n\")\n",
    "            f.write(\"=========================\\n\\n\")\n",
    "            \n",
    "            # Count topics by dimension\n",
    "            dimension_counts = {}\n",
    "            for info in topic_dimension_mapping.values():\n",
    "                for dim in info['matched_dimensions']:\n",
    "                    dimension_counts[dim] = dimension_counts.get(dim, 0) + 1\n",
    "            \n",
    "            f.write(\"Topics per Pain Dimension:\\n\")\n",
    "            for dim, count in dimension_counts.items():\n",
    "                f.write(f\"{dim}: {count} topics\\n\")\n",
    "            \n",
    "            # Count potentially new dimensions\n",
    "            new_dims = sum(1 for info in topic_dimension_mapping.values() \n",
    "                         if info['unknown_dimension'])\n",
    "            f.write(f\"\\nPotential New Dimensions: {new_dims} topics\\n\")\n",
    "            \n",
    "            # Temporal analysis\n",
    "            f.write(\"\\nTemporal Distribution:\\n\")\n",
    "            context_df['month_year'] = pd.to_datetime(context_df['created_date_cleaned']).dt.to_period('M')\n",
    "            temporal_dist = context_df.groupby('month_year').size()\n",
    "            f.write(temporal_dist.to_string())\n",
    "    def create_topic_model(self, \n",
    "                          docs: List[str],\n",
    "                          min_cluster_size: int = 50,\n",
    "                          n_neighbors: int = 10,\n",
    "                          n_components: int = 2,\n",
    "                          min_distance: float = 0.1) -> Tuple[BERTopic, np.ndarray, List[int]]:\n",
    "        \n",
    "        embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        embeddings = embedding_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "        umap_model = UMAP(\n",
    "            n_neighbors=n_neighbors,\n",
    "            n_components=n_components,\n",
    "            min_dist=min_distance,\n",
    "            metric='cosine'\n",
    "        )\n",
    "\n",
    "        hdbscan_model = HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            metric='euclidean',\n",
    "            cluster_selection_method='eom',\n",
    "            prediction_data=True\n",
    "        )\n",
    "\n",
    "        vectorizer_model = CountVectorizer(\n",
    "            stop_words=\"english\",\n",
    "            min_df=2,\n",
    "            ngram_range=(1, 4)\n",
    "        )\n",
    "\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model\n",
    "        )\n",
    "\n",
    "        topics, _ = topic_model.fit_transform(docs, embeddings)\n",
    "        \n",
    "        return topic_model, embeddings, topics\n",
    "\n",
    "    def save_visualizations(self, topic_model: BERTopic, docs: List[str], embeddings: np.ndarray):\n",
    "        base_filename = os.path.join(self.output_folder, \"pain_topic_analysis\")\n",
    "        \n",
    "        # Save interactive visualizations\n",
    "        topic_vis = topic_model.visualize_topics()\n",
    "        topic_vis.write_html(f\"{base_filename}_topic_visualization.html\")\n",
    "\n",
    "        doc_vis = topic_model.visualize_documents(docs, embeddings=embeddings)\n",
    "        doc_vis.write_html(f\"{base_filename}_document_visualization.html\")\n",
    "\n",
    "        barchart = topic_model.visualize_barchart(top_n_topics=50)\n",
    "        barchart.write_html(f\"{base_filename}_barchart.html\")\n",
    "\n",
    "        heatmap = topic_model.visualize_heatmap()\n",
    "        heatmap.write_html(f\"{base_filename}_heatmap.html\")\n",
    "\n",
    "        hierarchical_tree = topic_model.visualize_hierarchy()\n",
    "        hierarchical_tree.write_html(f\"{base_filename}_hierarchy.html\")\n",
    "\n",
    "    def calculate_topic_coherence(self, topic_model: BERTopic, docs: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Calculate coherence scores for topics and add interpretation.\"\"\"\n",
    "        # Extract topics and their words\n",
    "        topics = topic_model.get_topics()\n",
    "        topic_words = [[word for word, _ in topic_model.get_topic(topic_id)] for topic_id in topics.keys() if topic_id != -1 ]\n",
    "        \n",
    "        # Preprocess documents for Gensim\n",
    "        tokenized_docs = [doc.split() for doc in docs]\n",
    "        dictionary = corpora.Dictionary(tokenized_docs)\n",
    "        corpus = [dictionary.doc2bow(text) for text in tokenized_docs]\n",
    "    \n",
    "        # Calculate coherence\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topic_words, \n",
    "            texts=tokenized_docs, \n",
    "            dictionary=dictionary, \n",
    "            coherence='c_v'  # Use 'c_v' for human-interpretative coherence\n",
    "        )\n",
    "        coherence_scores = coherence_model.get_coherence_per_topic()\n",
    "    \n",
    "        # Define interpretation based on coherence score thresholds\n",
    "        def interpret_coherence(score):\n",
    "            if score >= 0.7:\n",
    "                return \"Highly Coherent\"\n",
    "            elif score >= 0.4:\n",
    "                return \"Moderately Coherent\"\n",
    "            else:\n",
    "                return \"Low Coherence\"\n",
    "    \n",
    "        # Combine topic IDs, coherence scores, and interpretations into a DataFrame\n",
    "        coherence_df = pd.DataFrame({\n",
    "            'Topic': [topic_id for topic_id in topics.keys() if topic_id != -1],\n",
    "            'Coherence_Score': coherence_scores,\n",
    "            'Interpretation': [interpret_coherence(score) for score in coherence_scores]\n",
    "        })\n",
    "        \n",
    "        # Save coherence scores to file\n",
    "        output_file = os.path.join(self.output_folder, \"topic_coherence_scores.csv\")\n",
    "        coherence_df.to_csv(output_file, index=False)\n",
    "        print(f\"Coherence scores with interpretations saved to {output_file}\")\n",
    "    \n",
    "        return coherence_df\n",
    "\n",
    "    def analyze_topics(self, topic_model: BERTopic, context_df: pd.DataFrame, \n",
    "                      docs: List[str], embeddings: np.ndarray, topics: List[int]):\n",
    "        base_filename = os.path.join(self.output_folder, \"pain_topic_analysis\")\n",
    "\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        print(\"\\nTopic Information:\")\n",
    "        # print(topic_info)\n",
    "        topic_df = pd.DataFrame(topic_info)\n",
    "        display(topic_df)\n",
    "        topic_df.to_csv(f\"{base_filename}.csv\",index=False)\n",
    "\n",
    "        self.save_visualizations(topic_model, docs, embeddings)\n",
    "        context_df['topic'] = topics\n",
    "        coherence_df = self.calculate_topic_coherence(topic_model, docs)\n",
    "        print(\"\\nTopic Coherence Scores:\")\n",
    "        display(coherence_df)\n",
    "        \n",
    "        topic_words = {}\n",
    "        for topic in set(topics):\n",
    "            if topic != -1:\n",
    "                words = topic_model.get_topic(topic)\n",
    "                topic_words[topic] = [word[0] for word in words]\n",
    "        topic_dimension_mapping = self.map_topics_to_dimensions(topic_words)\n",
    "        \n",
    "        self.save_analysis_results(topic_words, topic_dimension_mapping, \n",
    "                                 context_df, base_filename)\n",
    "        context_df = context_df.dropna(subset=['post_date'])\n",
    "        context_df['post_date'] = pd.to_datetime(context_df['post_date'], errors='coerce')\n",
    "        context_df = context_df.dropna(subset=['post_date'])\n",
    "        valid_indices = context_df.index\n",
    "        filtered_docs = [docs[i] for i in valid_indices]\n",
    "        filtered_topics = [topics[i] for i in valid_indices]\n",
    "        filtered_timestamps = context_df['post_date'].tolist()\n",
    "        print(\"Creating dynamic visualization of topic trends over time...\")\n",
    "        topics_over_time = topic_model.topics_over_time(\n",
    "            filtered_docs, filtered_timestamps, filtered_topics, nr_bins=20)\n",
    "        timeline_vis = topic_model.visualize_topics_over_time(topics_over_time)\n",
    "        timeline_vis.write_html(f\"{base_filename}_topics_over_time.html\")\n",
    "        print(\"Dynamic visualization saved successfully.\")\n",
    "        \n",
    "def main():\n",
    "    analyzer = PainTopicAnalyzer()\n",
    "    \n",
    "    # Load data\n",
    "    context_df, docs = analyzer.load_and_prepare_data(\n",
    "        \"reddit-data/matched-datasets/pain_lexicon_matched_subreddit_lupus_posts_full.csv\")\n",
    "    topic_model, embeddings, topics = analyzer.create_topic_model(docs)    \n",
    "    analyzer.analyze_topics(topic_model, context_df, docs, embeddings, topics)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
