{"cells":[{"cell_type":"code","execution_count":null,"id":"uQQPWP0xpAfy","metadata":{"id":"uQQPWP0xpAfy"},"outputs":[],"source":["import os\n","# Memory saving\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n"]},{"cell_type":"code","execution_count":null,"id":"2f0dbe72-c765-42d8-91ca-41644d8e9d69","metadata":{"id":"2f0dbe72-c765-42d8-91ca-41644d8e9d69"},"outputs":[],"source":["# mount drive to access data\n","from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"id":"c19a53d3-4a91-4551-bb63-ced959111040","metadata":{"id":"c19a53d3-4a91-4551-bb63-ced959111040"},"outputs":[],"source":["%cd /content/drive/MyDrive/Lupus-Subreddit-LLM/\n","%ls"]},{"cell_type":"code","source":["import re\n","import pandas as pd\n","import numpy as np"],"metadata":{"id":"z0Q2oqFx_1Nz"},"id":"z0Q2oqFx_1Nz","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"DPHubo3EXjZU","metadata":{"id":"DPHubo3EXjZU"},"source":["# Load Subreddit Annotated Dataset\n","\n","* Structured summaries based on concepts framed in the Biopsychosocial Model"]},{"cell_type":"code","execution_count":null,"id":"6dfcaf0b-3d00-48e6-9bfc-2da969a99403","metadata":{"id":"6dfcaf0b-3d00-48e6-9bfc-2da969a99403"},"outputs":[],"source":["# Read in subreddit lupus dataset which has been human annotated with json headers across columns\n","final_post_df = pd.read_csv(\"formatted_500_llm_lupus_final_set.csv\")\n","pd.set_option('display.max_colwidth', None)\n","# Drop unnecessary columns from final_post_df\n","final_post_df = final_post_df.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'], errors='ignore')\n","# Select only 'instruction' and 'response' from final_post_df\n","final_post_df = final_post_df[['instruction', 'response']]\n","\n","final_post_df.head(5)"]},{"cell_type":"code","source":["annotated_post_df = pd.read_csv(\"llm_finetuning_25_sample.csv\")\n","annotated_post_df = annotated_post_df[['selftext', 'labeled_summaries']].rename(columns={\n","    'selftext': 'instruction',\n","    'labeled_summaries': 'response'\n","})\n","annotated_post_df.head(5)\n","\n","second_set_df = pd.read_csv(\"filtered_ratings_revised_dw.csv\")\n","\n","second_set_df = second_set_df[['instruction', 'response_edited']].rename(columns={\n","    'response_edited': 'response'\n","})\n","second_set_df.head(5)\n"],"metadata":{"id":"s5ho0-_Z9TDN"},"id":"s5ho0-_Z9TDN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Concatenate all datasets together\n","combined_df = pd.concat([final_post_df, annotated_post_df, second_set_df], ignore_index=True)\n","combined_df"],"metadata":{"id":"UfRJYCjM_Vly"},"id":"UfRJYCjM_Vly","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import pandas as pd\n","import re\n","import ast  # To safely convert stringified lists into real lists\n","\n","# Function to extract, clean, and expand JSON while handling stringified lists\n","def extract_json_and_expand(response_text):\n","    if not isinstance(response_text, str):  # Ensure input is a string\n","        return {}\n","\n","    # Extract JSON part using regex (capture from first { to first })\n","    json_match = re.search(r'\\{.*?\\}', response_text, re.DOTALL)  # Truncate at first }\n","\n","    if json_match:\n","        json_text = json_match.group()\n","\n","        # Ensure JSON ends with }, if not, add one\n","        if not json_text.endswith(\"}\"):\n","            json_text += \"}\"\n","\n","        try:\n","            # Load JSON as dictionary\n","            json_data = json.loads(json_text)\n","\n","            # Deduplicate keys by merging lists & convert stringified lists\n","            cleaned_data = {}\n","            for key, value in json_data.items():\n","                # Convert stringified lists into real lists\n","                if isinstance(value, str) and value.startswith(\"[\") and value.endswith(\"]\"):\n","                    try:\n","                        value = ast.literal_eval(value)  # Convert safely\n","                    except (ValueError, SyntaxError):\n","                        pass  # Keep as string if conversion fails\n","\n","                # Handle merging and deduplication\n","                if key in cleaned_data:  # If duplicate key exists\n","                    if isinstance(cleaned_data[key], list) and isinstance(value, list):\n","                        cleaned_data[key] = list(set(cleaned_data[key] + value))  # Merge and deduplicate lists\n","                    elif isinstance(cleaned_data[key], str) and isinstance(value, str):\n","                        cleaned_data[key] = cleaned_data[key] + \"; \" + value  # Concatenate strings\n","                    else:\n","                        cleaned_data[key] = value  # Keep latest valid value\n","                else:\n","                    cleaned_data[key] = value  # Store key-value pair\n","\n","            # Convert list values into comma-separated strings\n","            return {key: ', '.join(value) if isinstance(value, list) else value for key, value in cleaned_data.items()}\n","\n","        except json.JSONDecodeError:\n","            return {\"Unstructured_Response\": response_text}  # Store raw text if JSON fails\n","\n","    return {\"Unstructured_Response\": response_text}  # Store raw text if no valid JSON found\n","\n","\n","# Apply function to extract and expand JSON in 'response' column\n","expanded_json_df = combined_df['response'].apply(extract_json_and_expand).apply(pd.Series)\n","\n","# Count number of failed extractions BEFORE fixing Unstructured_Response\n","num_failed_before = expanded_json_df['Unstructured_Response'].notna().sum()\n","print(f\"Number of failed JSON extractions before cleaning Unstructured_Response: {num_failed_before}\")\n","\n","# Step 2: Process rows where Unstructured_Response is not NaN (fix manually labeled JSON)\n","def fix_unstructured_response(text):\n","    if isinstance(text, str) and text.startswith(\"{\") and text.endswith(\"}\"):\n","        try:\n","            # Convert string dictionary back to actual dictionary\n","            json_data = ast.literal_eval(text)  # Converts string into real dictionary\n","            cleaned_data = {}\n","\n","            for key, value in json_data.items():\n","                # Convert stringified lists into real lists\n","                if isinstance(value, str) and value.startswith(\"[\") and value.endswith(\"]\"):\n","                    try:\n","                        value = ast.literal_eval(value)  # Convert safely\n","                    except (ValueError, SyntaxError):\n","                        pass  # Keep as string if conversion fails\n","\n","                # Store cleaned values\n","                cleaned_data[key] = ', '.join(value) if isinstance(value, list) else value\n","\n","            return cleaned_data\n","        except (ValueError, SyntaxError):\n","            return {}  # If conversion fails, return empty dictionary\n","    return {}\n","\n","# Apply function to fix unstructured responses\n","fixed_unstructured_df = expanded_json_df['Unstructured_Response'].dropna().apply(fix_unstructured_response).apply(pd.Series)\n","\n","# Merge fixed unstructured responses back into the main dataframe\n","final_parsed_posts = pd.concat([combined_df, expanded_json_df, fixed_unstructured_df], axis=1)\n","\n","# Count number of failed extractions AFTER fixing Unstructured_Response\n","num_failed_after = final_parsed_posts['Unstructured_Response'].notna().sum()\n","print(f\"Number of failed JSON extractions after cleaning Unstructured_Response: {num_failed_after}\")"],"metadata":{"id":"bRABqTmYBRk0"},"id":"bRABqTmYBRk0","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(final_parsed_posts.info())\n"],"metadata":{"id":"JsKOGtwlCbt7"},"id":"JsKOGtwlCbt7","execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_parsed_posts"],"metadata":{"id":"RGseoDc8NJgm"},"id":"RGseoDc8NJgm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Identify columns with duplicate names\n","duplicate_columns = final_parsed_posts.columns[final_parsed_posts.columns.duplicated()].tolist()\n","\n","# Merge duplicates while keeping the most complete version\n","for col in set(duplicate_columns):\n","    matching_cols = final_parsed_posts.loc[:, final_parsed_posts.columns == col]\n","\n","    if not matching_cols.empty:\n","        # Merge columns by filling missing values\n","        final_parsed_posts[col] = matching_cols.bfill(axis=1).iloc[:, 0]\n","\n","        # Drop all duplicate occurrences, keeping only the first\n","        final_parsed_posts = final_parsed_posts.loc[:, ~final_parsed_posts.columns.duplicated()]\n","\n","print(\"Duplicate columns merged successfully!\")\n","\n","# Display final column count\n","print(f\"Final column count: {final_parsed_posts.shape[1]}\")\n","\n"],"metadata":{"id":"7GEr_cpgCqws"},"id":"7GEr_cpgCqws","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(final_parsed_posts.info())\n"],"metadata":{"id":"gbMPdeLBCui8"},"id":"gbMPdeLBCui8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(final_parsed_posts[['Unstructured_Response']].dropna().head(1))\n"],"metadata":{"id":"N_SaX5aHERjw"},"id":"N_SaX5aHERjw","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import pandas as pd\n","import ast  # Safely convert stringified lists into real lists\n","\n","# Function to convert Unstructured_Response string into a proper dictionary\n","def fix_unstructured_response(text):\n","    if isinstance(text, str) and text.startswith(\"{\") and text.endswith(\"}\"):\n","        try:\n","            # Convert the entire text into a real dictionary\n","            json_data = ast.literal_eval(text)  # Converts string into actual dictionary\n","\n","            cleaned_data = {}\n","            for key, value in json_data.items():\n","                # Convert empty lists represented as strings ('[]') into real empty lists\n","                if value == '[]':\n","                    cleaned_data[key] = ''\n","\n","                # Convert single strings and lists stored as strings properly\n","                elif isinstance(value, str):\n","                    if value.startswith(\"[\") and value.endswith(\"]\"):\n","                        try:\n","                            value = ast.literal_eval(value)  # Convert safely\n","                            cleaned_data[key] = ', '.join(value) if isinstance(value, list) else value\n","                        except (ValueError, SyntaxError):\n","                            cleaned_data[key] = value  # Keep as string if conversion fails\n","                    else:\n","                        cleaned_data[key] = value  # Keep as-is if already a clean string\n","                else:\n","                    cleaned_data[key] = value  # Store properly formatted value\n","\n","            return cleaned_data\n","        except (ValueError, SyntaxError):\n","            return {}  # Return empty dictionary if conversion fails\n","    return {}\n","\n","# Apply the function to Unstructured_Response\n","fixed_unstructured_df = final_parsed_posts['Unstructured_Response'].dropna().apply(fix_unstructured_response).apply(pd.Series)\n","\n","# Ensure correct alignment with existing columns before merging\n","expected_columns = [col for col in final_parsed_posts.columns if col not in ['Unstructured_Response']]\n","fixed_unstructured_df = fixed_unstructured_df.reindex(columns=expected_columns, fill_value='')\n","\n","# Merge the corrected parsed data back into the main dataframe\n","final_parsed_posts.update(fixed_unstructured_df)\n","\n"],"metadata":{"id":"tugTrBBuE6-M"},"id":"tugTrBBuE6-M","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(final_parsed_posts.info())\n","\n","\n","final_parsed_posts.tail(5)"],"metadata":{"id":"TMmZcy-9E7qi"},"id":"TMmZcy-9E7qi","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# parsed 553 posts"],"metadata":{"id":"G7lbg6goNky8"},"id":"G7lbg6goNky8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_parsed_posts.to_csv(\"final_summary_post_expanded.csv\")"],"metadata":{"id":"sr8097DfCFHx"},"id":"sr8097DfCFHx","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"oLhG3uealcr9"},"id":"oLhG3uealcr9"},{"cell_type":"code","source":["import pandas as pd\n","# Rename 'Sex/Gender' column to 'Sex_Gender' if it exists\n","final_parsed_posts = final_parsed_posts.rename(columns={'Sex/Gender': 'Sex_Gender'})\n","\n","# Identify the relevant columns (all columns between 'response' and 'Unstructured_Response')\n","columns_to_summarize = final_parsed_posts.loc[:, 'response':].columns.tolist()\n","columns_to_summarize.remove('response')  # Remove 'response' itself\n","columns_to_summarize.remove('Unstructured_Response')  # Remove 'Unstructured_Response'\n","\n","# Create a dictionary to store summary DataFrames for each column\n","summary_dfs = {}\n","\n","# Generate unique value counts for each column\n","for col in columns_to_summarize:\n","    value_counts = final_parsed_posts[col].dropna().str.split(', ').explode().value_counts().reset_index()\n","    value_counts.columns = ['Value', 'Frequency']\n","    value_counts['Themes'] = ''  # Blank column for themes\n","    summary_dfs[col] = value_counts\n","\n","    # Save to CSV\n","    csv_filename = f\"{col}_summary.csv\"\n","    value_counts.to_csv(csv_filename, index=False)\n","    print(f\"Saved: {csv_filename}\")\n","\n","print(\"All summaries have been saved as CSV files.\")\n"],"metadata":{"id":"OyPfd4tIlcGX"},"id":"OyPfd4tIlcGX","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZEFlF32Qj4RR"},"id":"ZEFlF32Qj4RR","execution_count":null,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"19V3MY6aWpB5UZuqj62gPXVOIIgyCzmNd","timestamp":1742397486065},{"file_id":"11wZE5rMqxELa-tmN9cOG4lDf-KzKN_2w","timestamp":1742308119479},{"file_id":"1xQkAgntBN3b0tusmRCuoWC_g6mpr5TTj","timestamp":1739481393609}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}